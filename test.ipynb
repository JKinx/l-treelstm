{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.distributions as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_lstm(lstm):\n",
    "    for parameter in lstm.named_parameters():\n",
    "        name = parameter[0]\n",
    "        if \"bias\" in name:\n",
    "            nn.init.constant_(parameter[1], val=5)\n",
    "        elif \"ih\" in name:\n",
    "            nn.init.xavier_uniform_(parameter[1])\n",
    "        elif \"hh\" in name:\n",
    "            nn.init.orthogonal_(parameter[1])\n",
    "        else:\n",
    "            raise ValueError(\"Problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def masked_softmax(logits, mask):\n",
    "    \"\"\"\n",
    "    This method will return valid probability distribution for the particular instance if its corresponding row\n",
    "    in the `mask` matrix is not a zero vector. Otherwise, a uniform distribution will be returned.\n",
    "    This is just a technical workaround that allows `Categorical` class usage.\n",
    "    If probs doesn't sum to one there will be an exception during sampling.\n",
    "    \"\"\"\n",
    "    probs = torch.softmax(logits, dim=-1) * mask\n",
    "    probs = probs + (mask.sum(dim=-1, keepdim=True) == 0.).to(dtype=torch.float32)\n",
    "    Z = probs.sum(dim=-1, keepdim=True)\n",
    "    return probs / Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2961, 0.1662, 0.1750, 0.2033, 0.1595, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0834, 0.0972, 0.0985, 0.1365, 0.0743, 0.1255, 0.1433, 0.0732, 0.0790,\n",
       "         0.0892],\n",
       "        [0.2369, 0.1900, 0.1127, 0.1553, 0.1450, 0.1600, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0707, 0.0924, 0.1398, 0.1145, 0.1153, 0.0928, 0.1309, 0.0908, 0.1529,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_softmax(logits, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gumbel_softmax(logits, temperature, mask=None):\n",
    "    epsilon = 1e-20\n",
    "    \n",
    "    # get gumbel noise\n",
    "    unif = ds.Uniform(0,1).sample(logits.size())\n",
    "    gumbel_noise = -(-(unif + epsilon).log() + epsilon).log()\n",
    "    \n",
    "    # get samples \n",
    "    new_logits = (logits + gumbel_noise) / temperature\n",
    "    if mask is None:\n",
    "        y = new_logits.softmax(dim=-1)\n",
    "    else:\n",
    "        y = masked_softmax(new_logits, mask)\n",
    "        \n",
    "    # hard samples\n",
    "    y_st = torch.zeros_like(y).scatter_(-1, y.argmax(dim=-1, keepdim=True), 1.0)\n",
    "    # sample with gradients\n",
    "    y = (y_st - y).detach() + y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_entropy(logits, mask):\n",
    "    probs = masked_softmax(logits, mask)\n",
    "    return -(probs.log() * probs * mask).sum(-1) * (mask.sum(-1) != 1.).float()\n",
    "\n",
    "def cat_norm_entropy(logits, mask):\n",
    "    log_n = (mask.sum(-1) + 1e-17).log()\n",
    "    entropy = cat_entropy(logits, mask)\n",
    "    return entropy / (log_n + 1e-17)\n",
    "\n",
    "def cat_logprob(logits, mask, values):\n",
    "    # values is one-hot encoded\n",
    "    lprobs = masked_softmax(logits, mask).log()\n",
    "    log_prob = torch.gather(lprobs, -1, values.argmax(-1, keepdim=True)).squeeze()\n",
    "    return log_prob * (mask.sum(-1) != 0.).float()\n",
    "\n",
    "def get_seqmask(seqlens):\n",
    "    # get sequence mask from seqlens\n",
    "    # output shape = batch X maxlen\n",
    "    maxlen = seqlens.max()\n",
    "    batch = seqlens.shape[0]\n",
    "    arange = torch.arange(maxlen).unsqueeze(0).expand(batch, -1).long().to(seqlens)\n",
    "    return (arange < seqlens.unsqueeze(-1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BTreeLSTMCell(nn.Module):\n",
    "    def __init__(self, hdim, dropout_prob=None):\n",
    "        super().__init__()\n",
    "        self.hdim = hdim\n",
    "        self.linear = nn.Linear(in_features = 2*self.hdim, out_features = 5*self.hdim)\n",
    "        if dropout_prob is None:\n",
    "            self.dropout = lambda x : x\n",
    "        else: \n",
    "            self.dropout = nn.Dropout(dropout_prob)  \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.orthogonal_(self.linear.weight)\n",
    "        nn.init.constant_(self.linear.bias, val = 0)\n",
    "        nn.init.constant_(self.linear.bias[self.hdim:3*self.hdim], val = 1)\n",
    "        \n",
    "    def forward(self, hl, cl, hr, cr):\n",
    "        h = torch.cat([hl, hr], dim=-1)\n",
    "        i, fl, fr, o, g = self.linear(h).chunk(chunks = 5, dim = -1)\n",
    "        cp = self.dropout(g.tanh_()) * i.sigmoid_() + cl * fl.sigmoid_() + cr * fr.sigmoid_()\n",
    "        hp = o.sigmoid() * cp.tanh()\n",
    "        return hp, cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 256]), torch.Size([5, 256]))"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = 5\n",
    "hdim = 256\n",
    "hl = torch.rand(batch, hdim)\n",
    "cl = torch.rand(batch, hdim)\n",
    "hr = torch.rand(batch, hdim)\n",
    "cr = torch.rand(batch, hdim)\n",
    "cell = BTreeLSTMCell(hdim)\n",
    "cell(hl, cl, hr, cr)[0].shape, cell(hl, cl, hr, cr)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BTreeLSTMBase(nn.Module):\n",
    "    def __init__(self, idim, hdim, tdim, dropout_prob=None):\n",
    "        super().__init__()\n",
    "        self.leaftransformer_lstm = nn.LSTM(idim, tdim)\n",
    "        self.leaftransformer_linear = nn.Linear(tdim, 2*hdim)\n",
    "        \n",
    "        self.treelstm_cell = BTreeLSTMCell(hdim, dropout_prob)\n",
    "        \n",
    "        BTreeLSTMBase.reset_parameters(self)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.orthogonal_(self.leaftransformer_linear.weight)\n",
    "        nn.init.constant_(self.leaftransformer_linear.bias, val=0)\n",
    "        self.treelstm_cell.reset_parameters()\n",
    "        self.leaftransformer_lstm.reset_parameters()\n",
    "    \n",
    "    def transform_leafs(self, x):\n",
    "        # x : Shape = batch X seqlen X idim\n",
    "        x = self.leaftransformer_lstm(x)[0]\n",
    "        # Shape = batch X seqlen X 2*hdim\n",
    "        x = self.leaftransformer_linear(x).tanh()\n",
    "        # Shape = (batch X seqlen X hdim, batch X seqlen X hdim)\n",
    "        return x.chunk(chunks=2, dim=-1)\n",
    "    \n",
    "    def compose(self, composition, hl, cl, hr, cr, hp, cp, mask):\n",
    "        # composition : Shape = batch X seqlen\n",
    "        # hl, hr, hp, cl, cr, cp : Shape = batch X seqlen X hdim\n",
    "        # mask : Shape = batch X seqlen\n",
    "        # mask is for padding\n",
    "        cumsum = torch.cumsum(composition, dim=-1)\n",
    "        \n",
    "        # Shape = batch X maxlen X 1\n",
    "        # for broadcasting\n",
    "        ml = (1 - cumsum).unsqueeze(-1)\n",
    "        mr = (cumsum - 1).unsqueeze(-1)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        composition = composition.unsqueeze(-1)\n",
    "        hp = mask * (ml * hl + mr * hr + composition * hp) + (1 - mask) * hl\n",
    "        cp = mask * (ml * cl + mr * cr + composition * cp) + (1 - mask) * cl\n",
    "        return hp, cp\n",
    "    \n",
    "    def forward(self, *inputs):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 10, 256]), torch.Size([5, 10, 256]))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = 5\n",
    "seqlen = 10\n",
    "idim = 100\n",
    "tdim = 300\n",
    "hdim = 256\n",
    "x = torch.rand(batch, seqlen, idim)\n",
    "base = BTreeLSTMBase(idim, hdim, tdim)\n",
    "base.transform_leafs(x)[0].shape, base.transform_leafs(x)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BTreeLSTMParser(BTreeLSTMBase):\n",
    "    def __init__(self, idim, hdim, tdim, gumbel_temperature, dropout_prob=None):\n",
    "        super().__init__(idim, hdim, tdim, dropout_prob)\n",
    "        self.q = nn.Parameter(torch.FloatTensor(hdim))\n",
    "        # temperature for gumbel softmax\n",
    "        self.gumbel_temperature = gumbel_temperature\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        nn.init.normal_(self.q, mean=0, std=0.01)\n",
    "    \n",
    "    def sample_composition(self, query_weights, mask):\n",
    "        if self.training:\n",
    "            # sample from gumbel_softmax if training\n",
    "            composition = gumbel_softmax(query_weights, self.gumbel_temperature, mask)\n",
    "        else:\n",
    "            # greedy if not\n",
    "            logits = masked_softmax(query_weights, mask)\n",
    "            composition = torch.zeros_like(logits).scatter_(-1, logits.argmax(dim=-1, keepdim=True), 1.0)\n",
    "        return composition\n",
    "    \n",
    "    def step(self, h, c, mask, eval_composition):\n",
    "        # get left and right sides\n",
    "        hl, hr = h[:,:-1], h[:,1:]\n",
    "        cl, cr = c[:,:-1], c[:,1:]\n",
    "        # composed states\n",
    "        hp, cp = self.treelstm_cell(hl, cl, hr, cr)\n",
    "        \n",
    "        # get composition query weights\n",
    "        query_weights = torch.matmul(hp, self.q)\n",
    "        if eval_composition is None:\n",
    "            # sample is not given\n",
    "            composition = self.sample_composition(query_weights, mask)\n",
    "        else:\n",
    "            # use provided mergers if available\n",
    "            composition = eval_composition\n",
    "            \n",
    "        # perform merge action\n",
    "        hp, cp = self.compose(composition, hl, cl, hr, cr, hp, cp, mask)\n",
    "        return hp, cp, composition, query_weights\n",
    "        \n",
    "    def forward(self, x, mask, eval_tree_compositions=None):\n",
    "        # transform the leafs\n",
    "        h, c = self.transform_leafs(x)\n",
    "        \n",
    "        # values to record\n",
    "        entropy = []\n",
    "        norm_entropy = []\n",
    "        log_probs = []\n",
    "        tree_compositions = []\n",
    "        hs = [h]\n",
    "        cs = [c]\n",
    "        for i in range(x.shape[1]-1):\n",
    "            # get the relevant mask (1 less than the pervious one)\n",
    "            rel_mask = mask[:, i+1:]\n",
    "            # perfrom a step (move up a layer)\n",
    "            eval_composition = None if eval_tree_compositions is None else eval_tree_compositions[i]\n",
    "            h, c, composition, query_weights = self.step(h, c, rel_mask, eval_composition)\n",
    "            tree_compositions.append(composition)\n",
    "            entropy.append(cat_entropy(query_weights, rel_mask))\n",
    "            norm_entropy.append(cat_norm_entropy(query_weights, rel_mask))\n",
    "            log_probs.append(cat_logprob(query_weights, rel_mask, composition))\n",
    "            hs.append(h)\n",
    "            cs.append(c)\n",
    "            \n",
    "        entropy = sum(entropy)\n",
    "        norm_entropy = sum(norm_entropy) / (mask[:, 2:].sum(-1) + 1e-17)\n",
    "        log_probs = sum(log_probs)\n",
    "        \n",
    "        return tree_compositions, log_probs, entropy, norm_entropy, hs, cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 5\n",
    "seqlen = 10\n",
    "idim = 100\n",
    "tdim = 300\n",
    "hdim = 256\n",
    "temperature = 1\n",
    "seqlens = torch.tensor([5,10,6,1,9])\n",
    "mask = get_seqmask(seqlens)\n",
    "logits = torch.rand(batch, seqlen)\n",
    "x = torch.rand(batch, seqlen, idim)\n",
    "\n",
    "parser = BTreeLSTMParser(idim, hdim, tdim, temperature)\n",
    "parser.train()\n",
    "\n",
    "tc, lp, e, ne, h, c = parser(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2048)\n",
      "tensor(1792)\n",
      "tensor(1024)\n",
      "tensor(1024)\n",
      "tensor(1024)\n",
      "tensor(768)\n",
      "tensor(512)\n",
      "tensor(256)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "j = 4\n",
    "for k in range(seqlen-1):\n",
    "    print((1 - (h[k][j][:-1] == h[k+1][j])).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BTreeLSTMComposer(BTreeLSTMBase):\n",
    "    def __init__(self, idim, hdim, tdim, dropout_prob=None):\n",
    "        super().__init__(idim, hdim, tdim, dropout_prob)\n",
    "    \n",
    "    def forward(self, x, seqlens, tree_compositions):\n",
    "        # get sequence mask from seqlens\n",
    "        # mask : Shape = batch X maxlen\n",
    "        mask = self.get_seqmask(seqlens)\n",
    "        \n",
    "        # transform the leafs\n",
    "        h, c = self.transform_leafs(x, mask)\n",
    "        \n",
    "        # perform merges\n",
    "        for i in range(x.shape[1]-1):\n",
    "            hl, hr = h[:,:-1], h[:,1:]\n",
    "            cl, cr = c[:,:-1], c[:,1:]\n",
    "            hp, cp, self.treelstm_cell(hl, cl, hr, cr)\n",
    "            hp, cp = self.compose(tree_compositions[i], hl, cl, hr, cr, hp, cp, mask[:, i+1:])\n",
    "        \n",
    "        # return root\n",
    "        return hp.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, vocab_size, idim, hdim, p_tdim, c_tdim, odim, gumbel_temperature):\n",
    "        super().__init_()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, idim)\n",
    "        self.parser = BTreeLSTMParser(idim, hdim, p_tdim, gumbel_temperature)\n",
    "        self.tree_embeddings = nn.Embedding(vocab_size, idim)\n",
    "        self.composer = BTreeLSTMComposer(idim, hdim, c_tdim)\n",
    "        self.linear = nn.Linear(hdim, odim)\n",
    "        \n",
    "        self.running_reward_var = 1.0\n",
    "        self.norm_alpha = 0.9\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.word_embeddings.weight, 0.0, 0.01)\n",
    "        nn.init.normal_(self.tree_embeddings.weight, 0.0, 0.01)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        nn.init.constant_(self.linear.bias, val=0)\n",
    "        self.parser.reset_parameters()\n",
    "        self.composer.reset_parameters()\n",
    "        \n",
    "    def single_pass(self, x, mask, labels):\n",
    "        tree_compositions, log_probs, entropy, norm_entropy = self.parser(self.word_embeddings(x), mask)\n",
    "        out = self.composer(self.tree_embeddings(x), mask, compositions)\n",
    "        logits = self.linear(x)\n",
    "        rewards = self.criterion(logits, labels)\n",
    "        return tree_compositions, log_probs, entropy, norm_entropy, rewards\n",
    "    \n",
    "    def get_baseline(self, x, mask, labels):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            rewards_c = single_pass(x, mask, labels)[-1]\n",
    "            self.train()\n",
    "            return rewards\n",
    "    \n",
    "    def normalize_rewards(self, rewards):\n",
    "        with torch.no_grad():\n",
    "            self.running_reward_var = self.norm_alpha * self.running_reward_var + \\\n",
    "                                        (1 - self.norm_alpha) * rewards.var()\n",
    "            return rewards / self.running_reward_var.sqrt().clamp(min=1.0)\n",
    "         \n",
    "    def forward(self, x, mask, labels):\n",
    "        tree_compositions, log_probs, entropy, norm_entropy, rewards =  single_pass(x, mask, labels)\n",
    "        if self.training:\n",
    "            baseline = self.get_baseline(x, mask, labels)\n",
    "            rewards = self.normalize_rewards(rewards - baseline)\n",
    "        loss = rewards.mean().detach\n",
    "        predictions = log_probs.argmax(dim=-1)\n",
    "        return predictions, tree_compositions, loss, rewards, log_probs, entropy, norm_entropy\n",
    "    \n",
    "    def evaluate(self, x, mask, eval_tree_compositions):\n",
    "        _, log_probs, _, norm_entropy = self.parser(self.word_embeddings(x), mask, eval_tree_compositions)\n",
    "        return log_probs, norm_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
