{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.distributions as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_lstm(lstm):\n",
    "    for parameter in lstm.named_parameters():\n",
    "        name = parameter[0]\n",
    "        if \"bias\" in name:\n",
    "            nn.init.constant_(parameter[1], val=5)\n",
    "        elif \"ih\" in name:\n",
    "            nn.init.xavier_uniform_(parameter[1])\n",
    "        elif \"hh\" in name:\n",
    "            nn.init.orthogonal_(parameter[1])\n",
    "        else:\n",
    "            raise ValueError(\"Problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def masked_softmax(logits, mask):\n",
    "    probs = torch.softmax(logits, dim=-1) * mask\n",
    "    probs = probs + (mask.sum(dim=-1, keepdim=True) == 0.).to(dtype=torch.float32)\n",
    "    Z = probs.sum(dim=-1, keepdim=True)\n",
    "    return probs / Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gumbel_softmax(logits, temperature, mask=None):\n",
    "    epsilon = 1e-20\n",
    "    \n",
    "    # get gumbel noise\n",
    "    unif = ds.Uniform(0,1).sample(logits.size())\n",
    "    gumbel_noise = -(-(unif + epsilon).log() + epsilon).log()\n",
    "    \n",
    "    # get samples \n",
    "    new_logits = (logits + gumbel_noise) / temperature\n",
    "    if mask is None:\n",
    "        y = new_logits.softmax(dim=-1)\n",
    "    else:\n",
    "        y = masked_softmax(new_logits, mask)\n",
    "        \n",
    "    # hard samples\n",
    "    y_st = torch.zeros_like(y).scatter_(-1, y.argmax(dim=-1, keepdim=True), 1.0)\n",
    "    # sample with gradients\n",
    "    y = (y_st - y).detach() + y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cat_entropy(logits, mask):\n",
    "    probs = masked_softmax(logits, mask) + 1e-17\n",
    "    entropy = -(probs.log() * probs * mask).sum(-1) * (mask.sum(-1) != 1.).float()\n",
    "    return entropy\n",
    "\n",
    "def cat_norm_entropy(logits, mask):\n",
    "    log_n = (mask.sum(-1) + 1e-17).log()\n",
    "    entropy = cat_entropy(logits, mask)\n",
    "    return entropy / (log_n + 1e-17)\n",
    "\n",
    "def cat_logprob(logits, mask, values):\n",
    "    # values is one-hot encoded\n",
    "    lprobs = masked_softmax(logits, mask).log()\n",
    "    log_prob = torch.gather(lprobs, -1, values.argmax(-1, keepdim=True)).squeeze()\n",
    "    return log_prob * (mask.sum(-1) != 0.).float()\n",
    "\n",
    "def get_seqmask(seqlens):\n",
    "    # get sequence mask from seqlens\n",
    "    # output shape = batch X maxlen\n",
    "    maxlen = seqlens.max()\n",
    "    batch = seqlens.shape[0]\n",
    "    arange = torch.arange(maxlen).unsqueeze(0).expand(batch, -1).long().to(seqlens)\n",
    "    return (arange < seqlens.unsqueeze(-1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BTreeLSTMCell(nn.Module):\n",
    "    def __init__(self, hdim, dropout_prob=None):\n",
    "        super().__init__()\n",
    "        self.hdim = hdim\n",
    "        self.linear = nn.Linear(in_features = 2*self.hdim, out_features = 5*self.hdim)\n",
    "        if dropout_prob is None:\n",
    "            self.dropout = lambda x : x\n",
    "        else: \n",
    "            self.dropout = nn.Dropout(dropout_prob)  \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.orthogonal_(self.linear.weight)\n",
    "        nn.init.constant_(self.linear.bias, val = 0)\n",
    "        nn.init.constant_(self.linear.bias[self.hdim:3*self.hdim], val = 1)\n",
    "        \n",
    "    def forward(self, hl, cl, hr, cr):\n",
    "        # h[], c[] : Shape = batch X seqlen X hdim\n",
    "        h = torch.cat([hl, hr], dim=-1)\n",
    "        i, fl, fr, o, g = self.linear(h).chunk(chunks = 5, dim = -1)\n",
    "        cp = self.dropout(g.tanh_()) * i.sigmoid_() + cl * fl.sigmoid_() + cr * fr.sigmoid_()\n",
    "        hp = o.sigmoid() * cp.tanh()\n",
    "        return hp, cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BTreeLSTMBase(nn.Module):\n",
    "    def __init__(self, idim, hdim, tdim, dropout_prob=None):\n",
    "        super().__init__()\n",
    "        self.leaftransformer_lstm = nn.LSTM(idim, tdim)\n",
    "        self.leaftransformer_linear = nn.Linear(tdim, 2*hdim)\n",
    "        \n",
    "        self.treelstm_cell = BTreeLSTMCell(hdim, dropout_prob)\n",
    "        \n",
    "        BTreeLSTMBase.reset_parameters(self)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.orthogonal_(self.leaftransformer_linear.weight)\n",
    "        nn.init.constant_(self.leaftransformer_linear.bias, val=0)\n",
    "        self.treelstm_cell.reset_parameters()\n",
    "        self.leaftransformer_lstm.reset_parameters()\n",
    "    \n",
    "    def transform_leafs(self, x):\n",
    "        # x : Shape = batch X seqlen X idim\n",
    "        x = self.leaftransformer_lstm(x)[0]\n",
    "        # Shape = batch X seqlen X 2*hdim\n",
    "        x = self.leaftransformer_linear(x).tanh()\n",
    "        # Shape = (batch X seqlen X hdim, batch X seqlen X hdim)\n",
    "        return x.chunk(chunks=2, dim=-1)\n",
    "    \n",
    "    def compose(self, composition, hl, cl, hr, cr, hp, cp, mask):\n",
    "        # composition : Shape = batch X seqlen\n",
    "        # hl, hr, hp, cl, cr, cp : Shape = batch X seqlen X hdim\n",
    "        # mask : Shape = batch X seqlen\n",
    "        # mask is for padding\n",
    "        cumsum = torch.cumsum(composition, dim=-1)\n",
    "        \n",
    "        # Shape = batch X maxlen X 1\n",
    "        # for broadcasting\n",
    "        ml = (1 - cumsum).unsqueeze(-1)\n",
    "        mr = (cumsum - 1).unsqueeze(-1)\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        composition = composition.unsqueeze(-1)\n",
    "        \n",
    "        # next layer\n",
    "        hp = mask * (ml * hl + mr * hr + composition * hp) + (1 - mask) * hl\n",
    "        cp = mask * (ml * cl + mr * cr + composition * cp) + (1 - mask) * cl\n",
    "        return hp, cp\n",
    "    \n",
    "    def forward(self, *inputs):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BTreeLSTMParser(BTreeLSTMBase):\n",
    "    def __init__(self, idim, hdim, tdim, gumbel_temperature, dropout_prob=None):\n",
    "        super().__init__(idim, hdim, tdim, dropout_prob)\n",
    "        self.q = nn.Parameter(torch.FloatTensor(hdim))\n",
    "        # temperature for gumbel softmax\n",
    "        self.gumbel_temperature = gumbel_temperature\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        nn.init.normal_(self.q, mean=0, std=0.01)\n",
    "    \n",
    "    def sample_composition(self, query_weights, mask):\n",
    "        if self.training:\n",
    "            # sample from gumbel_softmax if training\n",
    "            composition = gumbel_softmax(query_weights, self.gumbel_temperature, mask)\n",
    "        else:\n",
    "            # greedy if not\n",
    "            logits = masked_softmax(query_weights, mask)\n",
    "            composition = torch.zeros_like(logits).scatter_(-1, logits.argmax(dim=-1, keepdim=True), 1.0)\n",
    "        return composition\n",
    "    \n",
    "    def step(self, h, c, mask, eval_composition):\n",
    "        # get left and right sides\n",
    "        hl, hr = h[:,:-1], h[:,1:]\n",
    "        cl, cr = c[:,:-1], c[:,1:]\n",
    "        # composed states\n",
    "        hp, cp = self.treelstm_cell(hl, cl, hr, cr)\n",
    "        \n",
    "        # get composition query weights\n",
    "        query_weights = torch.matmul(hp, self.q)\n",
    "        if eval_composition is None:\n",
    "            # sample is not given\n",
    "            composition = self.sample_composition(query_weights, mask)\n",
    "        else:\n",
    "            # use provided mergers if available\n",
    "            composition = eval_composition\n",
    "            \n",
    "        # perform composition\n",
    "        hp, cp = self.compose(composition, hl, cl, hr, cr, hp, cp, mask)\n",
    "        return hp, cp, composition, query_weights\n",
    "        \n",
    "    def forward(self, x, mask, eval_tree_compositions=None):\n",
    "        # transform the leafs\n",
    "        h, c = self.transform_leafs(x)\n",
    "        \n",
    "        # values to record\n",
    "        entropy = []\n",
    "        norm_entropy = []\n",
    "        log_probs = []\n",
    "        tree_compositions = []\n",
    "        hs = [h]\n",
    "        cs = [c]\n",
    "        for i in range(x.shape[1]-1):\n",
    "            # get the relevant mask (1 less than the pervious one)\n",
    "            rel_mask = mask[:, i+1:]\n",
    "            # perfrom a step (move up a layer)\n",
    "            eval_composition = None if eval_tree_compositions is None else eval_tree_compositions[i]\n",
    "            h, c, composition, query_weights = self.step(h, c, rel_mask, eval_composition)\n",
    "            tree_compositions.append(composition)\n",
    "            entropy.append(cat_entropy(query_weights, rel_mask))\n",
    "            norm_entropy.append(cat_norm_entropy(query_weights, rel_mask))\n",
    "            log_probs.append(cat_logprob(query_weights, rel_mask, composition))\n",
    "            hs.append(h)\n",
    "            cs.append(c)\n",
    "            \n",
    "        entropy = sum(entropy)\n",
    "        norm_entropy = sum(norm_entropy) / (mask[:, 2:].sum(-1) + 1e-17)\n",
    "        log_probs = sum(log_probs)\n",
    "        \n",
    "        return tree_compositions, log_probs, entropy, norm_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BTreeLSTMComposer(BTreeLSTMBase):\n",
    "    def __init__(self, idim, hdim, tdim, dropout_prob=None):\n",
    "        super().__init__(idim, hdim, tdim, dropout_prob)\n",
    "    \n",
    "    def forward(self, x, mask, tree_compositions):        \n",
    "        # transform the leafs\n",
    "        h, c = self.transform_leafs(x)\n",
    "        \n",
    "        # perform merges\n",
    "        for i in range(x.shape[1]-1):\n",
    "            hl, hr = h[:,:-1], h[:,1:]\n",
    "            cl, cr = c[:,:-1], c[:,1:]\n",
    "            hp, cp = self.treelstm_cell(hl, cl, hr, cr)\n",
    "            h, c = self.compose(tree_compositions[i], hl, cl, hr, cr, hp, cp, mask[:, i+1:])\n",
    "        # return root\n",
    "        return h.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, idim, hdim, p_tdim, c_tdim, odim, gumbel_temperature):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, idim)\n",
    "        self.parser = BTreeLSTMParser(idim, hdim, p_tdim, gumbel_temperature)\n",
    "        self.tree_embeddings = nn.Embedding(vocab_size, idim)\n",
    "        self.composer = BTreeLSTMComposer(idim, hdim, c_tdim)\n",
    "        self.linear = nn.Linear(hdim, odim)\n",
    "        \n",
    "        self.running_reward_var = 1.0\n",
    "        self.norm_alpha = 0.9\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.word_embeddings.weight, 0.0, 0.01)\n",
    "        nn.init.normal_(self.tree_embeddings.weight, 0.0, 0.01)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        nn.init.constant_(self.linear.bias, val=0)\n",
    "        self.parser.reset_parameters()\n",
    "        self.composer.reset_parameters()\n",
    "        \n",
    "    def single_pass(self, x, mask, labels):\n",
    "        tree_compositions, log_probs, entropy, norm_entropy = self.parser(self.word_embeddings(x), mask)\n",
    "        out = self.composer(self.tree_embeddings(x), mask, tree_compositions)\n",
    "        logits = self.linear(out)\n",
    "        rewards = self.criterion(logits, labels)\n",
    "        return tree_compositions, log_probs, entropy, norm_entropy, logits, rewards\n",
    "    \n",
    "    def get_baseline(self, x, mask, labels):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            rewards_c = self.single_pass(x, mask, labels)[-1]\n",
    "            self.train()\n",
    "            return rewards_c\n",
    "    \n",
    "    def normalize_rewards(self, rewards):\n",
    "        with torch.no_grad():\n",
    "            self.running_reward_var = self.norm_alpha * self.running_reward_var + \\\n",
    "                                        (1 - self.norm_alpha) * rewards.var()\n",
    "            return rewards / self.running_reward_var.sqrt().clamp(min=1.0)\n",
    "         \n",
    "    def forward(self, x, mask, labels):\n",
    "        tree_compositions, log_probs, entropy, norm_entropy, logits, rewards =  self.single_pass(x, mask, labels)\n",
    "        loss = rewards.mean()\n",
    "        if self.training:\n",
    "            baseline = self.get_baseline(x, mask, labels)\n",
    "            rewards = self.normalize_rewards(rewards - baseline)\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        return predictions, tree_compositions, loss, rewards.detach(), log_probs, entropy, norm_entropy\n",
    "    \n",
    "    def evaluate(self, x, mask, eval_tree_compositions):\n",
    "        _, log_probs, _, norm_entropy = self.parser(self.word_embeddings(x), mask, eval_tree_compositions)\n",
    "        return log_probs, norm_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = 5\n",
    "seqlen = 10\n",
    "vocab_size = 20\n",
    "idim = 100\n",
    "tdim = 300\n",
    "p_tdim = 256\n",
    "c_tdim = 150\n",
    "odim = 10\n",
    "gumbel_temperature = 1\n",
    "model = Model(vocab_size, idim, hdim, p_tdim, c_tdim, odim, gumbel_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[0,1,2,3,4,5,6,7,8,9],\n",
    "                  [12,18,1,0,16,1,1,2,3,3],\n",
    "                  [0,0,0,1,1,1,2,2,3,3],\n",
    "                  [1,1,1,1,1,5,5,5,5,5],\n",
    "                  [2,3,2,6,11,15,16,14,15,12]])\n",
    "mask = torch.tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
    "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
    "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]])\n",
    "labels = torch.tensor([1,2,4,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "predictions, tree_compositions, loss, rewards, log_probs, entropy, norm_entropy = model(x, mask, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_log_probs, eval_norm_entropy = model.evaluate(x, mask, tree_compositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
